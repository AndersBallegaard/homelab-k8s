{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Homelab","text":"<p>This is a very much work in progress repository for migrating all (or atleast most) of my homelab to k8s</p> <p>This repo is intentionally public to act as inspiration to others, and force me to think more about security</p>"},{"location":"#project-structure","title":"Project structure","text":"<ul> <li>For setup and cluster management see admin/readme.md</li> <li>For services running on the cluster see the cluster dir</li> </ul> <p>This page is autogenerated, do not edit it directly see this for more information</p>"},{"location":"apps/apps/","title":"APPS","text":"<p>This section describes the apps running on the cluster</p> <p>This page is autogenerated, do not edit it directly see this for more information</p>"},{"location":"info/docs/","title":"About this documentation","text":"<p>Parts of the documentation in this use templates, if markdown file is generated from a template a notice should be present at the bottom of the file.</p> <p>This automatic generation is used to prevent documentation getting out of sync when the same information is needed in multiple places, an example could be that the repo's readme.md file should be identical to the \"Home\" page in the documentation, this is achived by using a template for the readme.md importing the \"Home\" document</p>"},{"location":"info/docs/#how-to-add-to-this-documentation","title":"How to add to this documentation","text":""},{"location":"info/docs/#create-new-section","title":"Create new section","text":"<p>First create a directory for the section, and a jinja template file</p> <pre><code># Make directory and empty jinja file\nmkdir -p docs/example\ntouch docs/example/example.jinja\n\n# All files should include footer containing warning not to edit md files directly\necho '{{ '{% include \\\"docs/footer.section\\\" %}' }}' &gt;&gt; docs/example/example.jinja\n</code></pre> <p>This page should be added to the nav section of mkdocs.yaml, see the file or mkdocs documentation for how to do this</p>"},{"location":"info/docs/#add-page","title":"Add page","text":"<p>Create a new file inside the directory you want to add to. The file should follow the 'relevant_name.section' naming scheme. Treat this file as a markdown file, and add the content you want</p> <p>Once you are ready to include it in the documentation add a line to the directorys .jinja file, the line should look something like below</p> <pre><code>{{ '{% include \\\"example/world_domination.section\\\" %}' }}\n</code></pre> <p>Note, this should always be placed above the footer inclusion</p> <p>While there aren't many restrictions on the content in a section, it is expected that each section starts with an H1 header</p>"},{"location":"info/docs/#documentation-deployment","title":"Documentation deployment","text":"<p>Deployment to github pages is not automatic, in order to deploy, follow this process</p> <pre><code># The following commands are all from the root of the project\n\n# Build documentation\n./build_docs.sh \n\n# Source venv\nsource .venv/bin/activate\n\n# Run locally to validate changes before deploying\nmkdocs serve\n\n# Use mkdocs github pages feature to deploy\nmkdocs gh-deploy\n</code></pre>"},{"location":"infra/conformance_testing/","title":"Conformance testing","text":""},{"location":"infra/conformance_testing/#test-cluster-conformance","title":"Test cluster conformance","text":"<p>In order to run tests, from a terminal running in this directory run the following command</p> <p>Note: If conformane testing have previously been run on this k8s version, the test results will be overwriten</p> <pre><code>./run_test.sh\n</code></pre>"},{"location":"infra/deployment/prepare_admin_node/","title":"Prepare admin node","text":""},{"location":"infra/deployment/prepare_admin_node/#prepare-admin-node","title":"Prepare admin node","text":"<p>To convert an ubuntu machine to an admin node run this command. This script uses git over SSH, so make sure the admin node have ssh keys created, and those keys have been added to github</p> <p>The script does the following - Installs openTofu - Installs git - Downloads this repository - Installs talosctl - Installs kubectl - Installs helm - Installs flux</p> <pre><code>curl --proto '=https' --tlsv1.2 -fsSL https://raw.githubusercontent.com/AndersBallegaard/homelab-k8s/refs/heads/main/admin/prepare_admin_node.sh | bash\n</code></pre>"},{"location":"infra/deployment/prepare_proxmox/","title":"Prepare proxmox","text":""},{"location":"infra/deployment/prepare_proxmox/#prepare-proxmox-for-cloudinit","title":"Prepare proxmox for cloudinit","text":"<p>In case proxmox isn't ready for cloudinit of talos, this is how to set it up. Currently this is done from the proxmox UI as a one of thing</p> <pre><code># Download a talos image from the talos image factory, it needs to be a nocloud image\n# The image should be placed under the name \"talos-nocloud-amd64.iso\" in the shared vm datastore\n\n# Create a VM with the 1g disk on shared storage, and attach the iso. Ram and CPU doesn't matter\n# Don't start the VM\n# The vm MUST be named talos-template, and should have the ID 9100\n# CPU Type should be host for best performace\n# Disk size is intentionally below recormendations for faster template deployments, cloud init will resize to whatever the VM is speced for\n#\n# Convert VM to template\n\n</code></pre>"},{"location":"infra/deployment/setup_cluster/","title":"Setup cluster","text":""},{"location":"infra/deployment/setup_cluster/#provision-cluster","title":"Provision Cluster","text":"<pre><code># Set env variables\nexport TF_VAR_cloudflare_api_token=READ_WRITE_TOKEN\nexport TF_VAR_cloudflare_zone_id=DOMAIN_ZONEID\nexport TF_VAR_vm_user_username=anders\nexport TF_VAR_vm_user_sshkey=\"SSH PUBLIC ID for authentication\"\nexport TF_VAR_proxmox_username=USERNAME@pam\nexport TF_VAR_proxmox_password=PROXMOX_PASSWORD\nexport TF_VAR_proxmox_api_url=https://PROXMOX_SERVER:8006/api2/json\n\n\n# Initialize openTofu\ncd admin\ntofu init\n\n# Bootstrap the cluster\n./patch_infra.sh \n</code></pre>"},{"location":"infra/deployment/setup_cluster/#setup-cillium-cni","title":"Setup Cillium CNI","text":"<pre><code># Create BGP Password\nkubectl create secret generic -n kube-system --type=string bgp-auth-secret --from-literal=password=REPLACEWITHPASSWORD\n\n</code></pre>"},{"location":"infra/deployment/setup_cluster/#setup-fluxcd","title":"Setup fluxCD","text":"<pre><code>flux bootstrap github \\\n  --token-auth \\\n  --owner=andersballegaard \\\n  --repository=homelab-k8s \\\n  --branch=main \\\n  --path=cluster \\\n  --personal\n</code></pre>"},{"location":"infra/networking/overview/","title":"Overview","text":"<p>The kubernetes lab uses Cilium as the CNI, in this deployment Cilium is running IPv6 only in native-routing mode with BGP peerings to two virtual VYOS routers acting as essentially a mix between TOR switches and a CE router for my personal ASN AS201911.</p> <p>Cilium was chosen as the CNI due to the following reasons - Excelent BGP support - Interesting observability tools though Hubble - High performance due to eBPF - Great support for network policies - Support for advanced networking features i might want to checkout like srv6 in k8s - Built in gateway API and ingress</p> <p>See diagram below for a vistual representation</p> <p></p>"},{"location":"infra/networking/overview/#addressing","title":"Addressing","text":"<p>Due to running IPv6 only in my own ASN, i have assigned a /48 network to this k8s lab.</p> <p>Some of the plan looks the way it does due to previously being part of a shared /48 network with other lab infra, cleaning up the IP plan is todo point for a time where the cluster is down anyways.</p> <p>The addressing plan looks as follows</p> Prefix Location Use 2a0e:97c0:ae3::/48 Lab Assigned Aggregate 2a0e:97c0:ae3:c100::/112 Cluster K8S Service subnet 2a0e:97c0:ae3:c200::/56 Cluster K8S PodCIDR 2a0e:97c0:ae3:c401::/64 Cluster K8S Cilium LB IP pool 2a0e:97c0:ae3:fff0::/64 Vlan 2502 - K8S Node network Node IP's 2a0e:97c0:ae3:ffff::/64 Loopback aggregates Assigned Aggregate 2a0e:97c0:ae3:ffff::1/128 r01.k8s.srv6.dk MGMT Loopback 2a0e:97c0:ae3:ffff::2/128 r02.k8s.srv6.dk MGMT Loopback"},{"location":"infra/networking/overview/#cilium-configuration","title":"Cilium configuration","text":"<p>Cilium is installed doing setup of the cluster, for more information about that see Cluster setup and patch_infra.sh script</p> <p>For configuration related parameters, I am using a kustomization deployed by fluxcd to manage it. This choice have been made in order to have as much declaritive, easy to manage configuration as posible.</p>"},{"location":"infra/networking/overview/#loadbalancer-pools","title":"Loadbalancer pools","text":"<p>The cluster needs external IP's to assign to any service of the type loadBalancer, given i have an ipv6 only cluster with my own address space, the most obivious choice was to use global ipv6 in this case, but you can absolutly find other solutions if that's not posible for your setup.</p> <pre><code># cluster/infra/cilium/lb-ipam.yaml\napiVersion: \"cilium.io/v2alpha1\"\nkind: CiliumLoadBalancerIPPool\nmetadata:\n  name: \"lb-pool\"\nspec:\n  blocks:\n  - cidr: \"2a0e:97c0:ae3:c401::/64\"\n</code></pre>"},{"location":"infra/networking/overview/#bgp-configuration","title":"BGP Configuration","text":"<p>Let's look at the general BGP setup. There is quite a lot going on here, but in it basicly just does the following * Creates a BGP \"Cluster\" called cilium-bgp, and provisions it on all nodes * Specifies that the local-as is 65500 * Sets up peerings to R01.k8s.srv6.dk and R02.k8s.srv6.dk * Creates a CiliumBGPPeerConfig (Peer group) specifying that ipv6 unicast is the only family enabled, and setting a password</p> <pre><code># cluster/infra/cilium/bgp.yaml\napiVersion: cilium.io/v2alpha1\nkind: CiliumBGPClusterConfig\nmetadata:\n  name: cilium-bgp\n  namespace: kube-system\nspec:\n  nodeSelector:\n    matchLabels:\n      kubernetes.io/os: linux\n  bgpInstances:\n  - name: \"instance-65500\"\n    localASN: 65500\n    peers:\n    - name: \"r01\"\n      peerASN: 65666\n      peerAddress: 2a0e:97c0:ae3:fff0::1\n      peerConfigRef:\n        name: \"cilium-peer\"\n    - name: \"r02\"\n      peerASN: 65666\n      peerAddress: 2a0e:97c0:ae3:fff0::2\n      peerConfigRef:\n        name: \"cilium-peer\"\n\n---\napiVersion: cilium.io/v2alpha1\nkind: CiliumBGPPeerConfig\nmetadata:\n  name: cilium-peer\n  namespace: kube-system\nspec:\n  authSecretRef: bgp-auth-secret\n  timers:\n    holdTimeSeconds: 9\n    keepAliveTimeSeconds: 3\n  ebgpMultihop: 4\n  gracefulRestart:\n    enabled: true\n    restartTimeSeconds: 15\n  families:\n    - afi: ipv6\n      safi: unicast\n      advertisements:\n        matchLabels:\n          advertise: bgp\n</code></pre> <p>NOTE: This uses the BGP secret set during cluster setup, please make sure the password accually matches.</p> <p>NOTE: Under cluster setup anotate-nodes.sh was executed in the background, this script sets router-id's on a list of nodes. If some nodes won't peer check that a router-id have been set</p>"},{"location":"infra/networking/overview/#create-annoncements","title":"Create annoncements","text":"<p>Ok, we are not quite done with the BGP configuration, we still needs to specify what to announce. </p> <p>In my example due to running cilium in native-routing mode, i want to advertise everything i know. For this reason i am creating two CiliumBGPAdvertisement objects bellow advertising the following</p> <ul> <li>ExternalIP</li> <li>LoadBalancerIP</li> <li>ClusterIP</li> <li>PodCIDR</li> </ul> <pre><code># cluster/infra/cilium/advertisements.yaml\n---\napiVersion: cilium.io/v2alpha1\nkind: CiliumBGPAdvertisement\nmetadata:\n  labels:\n    advertise: bgp\n  name: bgp-advertisements\nspec:\n  advertisements:\n  - advertisementType: Service\n    selector:\n      matchExpressions:\n      - key: somekey  # For some reason this works, and things break if i remove it, no this key does not exist anywhere\n        operator: NotIn\n        values:\n        - never-used-value\n    service:\n      addresses:\n      - ExternalIP\n      - LoadBalancerIP\n      - ClusterIP\n---\napiVersion: cilium.io/v2alpha1\nkind: CiliumBGPAdvertisement\nmetadata:\n  name: bgp-advertisements-podcidr\n  labels:\n    advertise: bgp\nspec:\n  advertisements:\n    - advertisementType: \"PodCIDR\"\n      attributes:\n        communities:\n          standard: [ \"65000:99\" ]\n        localPreference: 99\n</code></pre>"},{"location":"infra/networking/overview/#building-the-kustomization","title":"Building the kustomization","text":"<p>Let's put it all togther in a simple kustomization</p> <pre><code># cluster/infra/cilium/kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nmetadata:\n  name: cillium-config\nresources:\n  - lb-ipam.yaml\n  - bgp.yaml\n  - advertisements.yaml\n</code></pre>"},{"location":"infra/networking/overview/#router-configuration","title":"Router configuration","text":"<p>See VYOS Configuration</p> <p>This page is autogenerated, do not edit it directly see this for more information</p>"},{"location":"infra/networking/vyos/","title":"Vyos","text":"<p>Two VYOS Routers are used to provide external connectivity to the cluster, eventually this should be automated using cloud-init for VYOS, but right now i can't be bothered, so this is how they are setup</p>"},{"location":"infra/networking/vyos/#specs","title":"Specs","text":"<p>Both of the virtual routers have these specs, and are being run on proxmox. The specs are probably subject to change depending on the volume of traffic. But even with a full IPv6 internet routing table being annouced to them from AS201911 they seem happy with these modest specs.</p> <ul> <li>1 core</li> <li>2GB Ram</li> <li>1 NIC in AS201911 Transit network</li> <li>1 NIC in K8S network</li> </ul>"},{"location":"infra/networking/vyos/#general-router-baseline","title":"General router baseline","text":"<ul> <li>Setup accounts</li> <li>Setup update checking</li> <li>Disable SSH Password auth</li> </ul> <pre><code># Unless you want me, and only me to login to your router, i would sudgest changing this key\nset system login user anders authentication public-keys JUMPHOST key 'AAAAB3NzaC1yc2EAAAADAQABAAABgQC8LOd5EgLmJiqWlikJMBNqn92c1TMyDYYJybmMRwu3ysOcf5jWwzVx1d6jZOPboH241Q3Rg59AtqJpFLEDis0myVvdLfcZa88DsjhfFUgDA4LMatItoUpqj2xKwJsTHKWjaSL5zVGv1rXOnMv3CA90Cye7NrEP11AxrLeCawpsQC/Gwcrc7JLoKrYK1bz2zPI9WwPonKhOUC0zfpAX9G9mj3Ybp7aogYAsBweoFw4X533usswFDkPZl9SYnetufEZ/XGgl3TFoDETsQ4Ddztazu9snPRwlIyAq2Lv1+tU2hc2lKu0AWv4zPehEdMz8dXodB1bfkD63Rpy004Naa+tboJnJQUUpfZqqVEpI0byp7t64suAU1IvmgZo02A3kg6m+HJuMU6k1hAbrgXttRWR/fRDSvdGydPiyi+8mSvVYrstcpH63mvMw2gVOkoDeBzP+ClejkCoTLuXSb38reAivG0j02FyyoWRrBumfBHJb6lC8hUHQi04DfX2B/G+0UVM='\nset system login user anders authentication public-keys JUMPHOST type 'ssh-rsa'\n\nset system name-server '2606:4700:4700::1111'\nset system update-check auto-check\nset system update-check url 'https://raw.githubusercontent.com/vyos/vyos-nightly-build/refs/heads/current/version.json'\n\nset service ssh disable-password-authentication\nset system domain-name 'k8s.srv6.dk'\n\nset interfaces ethernet eth0 description 'AS201911_TRANSIT'\nset interfaces ethernet eth1 description 'K8S_NETWORK'\n</code></pre>"},{"location":"infra/networking/vyos/#firewall-configuration","title":"Firewall configuration","text":"<p>Since these servers are technicly directly connected to the internet, a few firewall rules to help secure the servers are in order. These are the rules i have chosen to create for now, mixing security with ease of administration. Obiviously, you probably want some thigher rules in enviorments where security is more of a concern</p> <ul> <li>I have other firewalls for the rest of my ASN so i generally trust anything from one of my ASN's IP's</li> <li>I am ok with permitting all outgoing traffic from the k8s cluster</li> <li>I want to allow port 80 and 443 in from the entire internet, for services.</li> <li>Obiviously ICMPv6, BGP, link local should also be allowed given the setup</li> </ul> <pre><code>set firewall ipv6 input filter default-action 'drop'\nset firewall ipv6 input filter default-log\nset firewall ipv6 input filter rule 1 action 'accept'\nset firewall ipv6 input filter rule 1 description 'Allow everything from AS201911 (my networks)'\nset firewall ipv6 input filter rule 1 source address '2a0e:97c0:ae0::/44'\nset firewall ipv6 input filter rule 2 action 'accept'\nset firewall ipv6 input filter rule 2 description 'Accept BGP'\nset firewall ipv6 input filter rule 2 destination port '179'\nset firewall ipv6 input filter rule 2 protocol 'tcp'\nset firewall ipv6 input filter rule 3 action 'accept'\nset firewall ipv6 input filter rule 3 description 'Accept link-local'\nset firewall ipv6 input filter rule 3 source address 'fe80::/16'\nset firewall ipv6 input filter rule 4 action 'accept'\nset firewall ipv6 input filter rule 4 protocol 'ipv6-icmp'\nset firewall ipv6 input filter rule 10 action 'accept'\nset firewall ipv6 input filter rule 10 description 'Allow anything out from K8S'\nset firewall ipv6 input filter rule 10 inbound-interface name 'eth1'\nset firewall ipv6 input filter rule 30 action 'accept'\nset firewall ipv6 input filter rule 30 description 'Allow HTTP/HTTPS to K8S'\nset firewall ipv6 input filter rule 30 destination address '2a0e:97c0:ae3::/48'\nset firewall ipv6 input filter rule 30 destination port 'http,https'\nset firewall ipv6 input filter rule 30 protocol 'tcp'\n</code></pre>"},{"location":"infra/networking/vyos/#bgp-config","title":"BGP Config","text":"<p>The BGP config here is quite simple, create peerings to AS201911, and create a listen group to dynamicly allow in K8S nodes. There is no reason to send anything other than a default route to the nodes, so we do that and limit the memory consumption to have more room for containers.</p> <pre><code>set policy prefix-list6 DEFAULT rule 1 action 'permit'\nset policy prefix-list6 DEFAULT rule 1 prefix '::/0'\nset policy route-map EXPORT_DEFAULT rule 10 action 'permit'\nset policy route-map EXPORT_DEFAULT rule 10 match ipv6 address prefix-list 'DEFAULT'\nset policy route-map EXPORT_DEFAULT rule 20 action 'deny'\nset protocols bgp address-family ipv6-unicast redistribute connected\nset protocols bgp listen range 2a0e:97c0:ae3:fff0::/64 peer-group 'K8S'\nset protocols bgp neighbor 2a0e:97c0:ae0:102::1 address-family ipv6-unicast\nset protocols bgp neighbor 2a0e:97c0:ae0:102::1 remote-as '201911'\nset protocols bgp neighbor 2a0e:97c0:ae0:102::2 address-family ipv6-unicast\nset protocols bgp neighbor 2a0e:97c0:ae0:102::2 remote-as '201911'\nset protocols bgp peer-group K8S address-family ipv6-unicast route-map export 'EXPORT_DEFAULT'\nset protocols bgp peer-group K8S password 'REPLACEWITHPASSWORD'\nset protocols bgp peer-group K8S remote-as 'external'\nset protocols bgp system-as '65666'\n</code></pre>"},{"location":"infra/networking/vyos/#router-specific-configuration-for-r01k8ssrv6dk","title":"Router specific configuration for r01.k8s.srv6.dk","text":"<pre><code>set protocols bgp parameters router-id '127.0.1.1'\nset interfaces ethernet eth0 address '2a0e:97c0:ae0:102::c001/64'\nset interfaces ethernet eth1 address '2a0e:97c0:ae3:fff0::1/64'\nset interfaces loopback lo address '2a0e:97c0:ae3:ffff::1/128'\nset system host-name 'r01'\n\n</code></pre>"},{"location":"infra/networking/vyos/#router-specific-configuration-for-r02k8ssrv6dk","title":"Router specific configuration for r02.k8s.srv6.dk","text":"<p>To be build when the direction of everything is a bit more stable</p> <pre><code>set protocols bgp parameters router-id '127.0.1.2'\nset interfaces ethernet eth0 address '2a0e:97c0:ae0:102::c002/64'\nset interfaces ethernet eth1 address '2a0e:97c0:ae3:fff0::2/64'\nset interfaces loopback lo address '2a0e:97c0:ae3:ffff::2/128'\nset system host-name 'r02'\n</code></pre>"},{"location":"troubleshooting/port-forwarding/","title":"Port forwarding","text":"<p>For troubleshooting purposes it is often practical to port forward a service in k8s.</p> <p>This is also the only way to access certine admin interfaces that aren't exposed with an ingress</p>"},{"location":"troubleshooting/port-forwarding/#generic-service","title":"Generic service","text":"<p>This is the general way to expose any service with a port-forward. In a step to reduce the use of legacy IP, binding to ipv6 is forced in this example, note you might want to bind to ::1 instead of ::</p> <pre><code>kubectl -n NAMESPACE port-forward svc/SERVICE_NAME PORT:PORT --address=\"::\"\n</code></pre>"},{"location":"troubleshooting/port-forwarding/#capacitor","title":"Capacitor","text":"<p>If troubleshooting fluxcd, it might be nice to have a more visual representation of what services are in fluxcd.</p> <p>For this purpose a capacitor deployment exists, but isn't exposed via any ingress for security reasons, to forward it run the following command</p> <pre><code>kubectl -n flux-system port-forward svc/capacitor 9000:9000 --address=\"::1\"\n</code></pre>"},{"location":"troubleshooting/port-forwarding/#ceph-dashboard","title":"CEPH dashboard","text":"<p>When troubleshooting ceph/rook, it might be nice to use the ceph dashboard, it can be exposed using the following command</p> <pre><code>kubectl -n rook-ceph port-forward svc/rook-ceph-mgr-dashboard 7000:7000 --address=\"::\"\n</code></pre> <p>In order to login use the username \"admin\", and the password provided by the following command.</p> <pre><code>kubectl get secret -n rook-ceph -o jsonpath='{.data.password}' rook-ceph-dashboard-password | base64 -d\n</code></pre>"}]}