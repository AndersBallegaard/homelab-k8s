{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Homelab This is a very much work in progress repository for migrating all (or atleast most) of my homelab to k8s This repo is intentionally public to act as inspiration to others, and force me to think more about security Project structure For setup and cluster management see admin/readme.md For services running on the cluster see the cluster dir This page is autogenerated, do not edit it directly see this for more information","title":"Home"},{"location":"#homelab","text":"This is a very much work in progress repository for migrating all (or atleast most) of my homelab to k8s This repo is intentionally public to act as inspiration to others, and force me to think more about security","title":"Homelab"},{"location":"#project-structure","text":"For setup and cluster management see admin/readme.md For services running on the cluster see the cluster dir This page is autogenerated, do not edit it directly see this for more information","title":"Project structure"},{"location":"apps/apps/","text":"APPS This section describes the apps running on the cluster This page is autogenerated, do not edit it directly see this for more information","title":"Apps"},{"location":"apps/apps/#apps","text":"This section describes the apps running on the cluster This page is autogenerated, do not edit it directly see this for more information","title":"APPS"},{"location":"info/docs/","text":"About this documentation Parts of the documentation in this use templates, if markdown file is generated from a template a notice should be present at the bottom of the file. This automatic generation is used to prevent documentation getting out of sync when the same information is needed in multiple places, an example could be that the repo's readme.md file should be identical to the \"Home\" page in the documentation, this is achived by using a template for the readme.md importing the \"Home\" document How to add to this documentation Create new section First create a directory for the section, and a jinja template file # Make directory and empty jinja file mkdir -p docs/example touch docs/example/example.jinja # All files should include footer containing warning not to edit md files directly echo '{{ '{% include \\\"docs/footer.section\\\" %}' }}' >> docs/example/example.jinja This page should be added to the nav section of mkdocs.yaml, see the file or mkdocs documentation for how to do this Add page Create a new file inside the directory you want to add to. The file should follow the 'relevant_name.section' naming scheme. Treat this file as a markdown file, and add the content you want Once you are ready to include it in the documentation add a line to the directorys .jinja file, the line should look something like below {{ '{% include \\\"example/world_domination.section\\\" %}' }} Note, this should always be placed above the footer inclusion While there aren't many restrictions on the content in a section, it is expected that each section starts with an H1 header Documentation deployment Deployment to github pages is not automatic, in order to deploy, follow this process # The following commands are all from the root of the project # Build documentation ./build_docs.sh # Source venv source .venv/bin/activate # Run locally to validate changes before deploying mkdocs serve # Use mkdocs github pages feature to deploy mkdocs gh-deploy","title":"Documentation"},{"location":"info/docs/#about-this-documentation","text":"Parts of the documentation in this use templates, if markdown file is generated from a template a notice should be present at the bottom of the file. This automatic generation is used to prevent documentation getting out of sync when the same information is needed in multiple places, an example could be that the repo's readme.md file should be identical to the \"Home\" page in the documentation, this is achived by using a template for the readme.md importing the \"Home\" document","title":"About this documentation"},{"location":"info/docs/#how-to-add-to-this-documentation","text":"","title":"How to add to this documentation"},{"location":"info/docs/#create-new-section","text":"First create a directory for the section, and a jinja template file # Make directory and empty jinja file mkdir -p docs/example touch docs/example/example.jinja # All files should include footer containing warning not to edit md files directly echo '{{ '{% include \\\"docs/footer.section\\\" %}' }}' >> docs/example/example.jinja This page should be added to the nav section of mkdocs.yaml, see the file or mkdocs documentation for how to do this","title":"Create new section"},{"location":"info/docs/#add-page","text":"Create a new file inside the directory you want to add to. The file should follow the 'relevant_name.section' naming scheme. Treat this file as a markdown file, and add the content you want Once you are ready to include it in the documentation add a line to the directorys .jinja file, the line should look something like below {{ '{% include \\\"example/world_domination.section\\\" %}' }} Note, this should always be placed above the footer inclusion While there aren't many restrictions on the content in a section, it is expected that each section starts with an H1 header","title":"Add page"},{"location":"info/docs/#documentation-deployment","text":"Deployment to github pages is not automatic, in order to deploy, follow this process # The following commands are all from the root of the project # Build documentation ./build_docs.sh # Source venv source .venv/bin/activate # Run locally to validate changes before deploying mkdocs serve # Use mkdocs github pages feature to deploy mkdocs gh-deploy","title":"Documentation deployment"},{"location":"infra/conformance_testing/","text":"Conformance testing Test cluster conformance In order to run tests, from a terminal running in this directory run the following command Note: If conformane testing have previously been run on this k8s version, the test results will be overwriten ./run_test.sh","title":"Conformance testing"},{"location":"infra/conformance_testing/#conformance-testing","text":"","title":"Conformance testing"},{"location":"infra/conformance_testing/#test-cluster-conformance","text":"In order to run tests, from a terminal running in this directory run the following command Note: If conformane testing have previously been run on this k8s version, the test results will be overwriten ./run_test.sh","title":"Test cluster conformance"},{"location":"infra/fluxcd/","text":"","title":"FluxCD"},{"location":"infra/kubernetes/","text":"","title":"K8S"},{"location":"infra/traefik/","text":"","title":"Traefik"},{"location":"infra/deployment/prepare_admin_node/","text":"Prepare admin node To convert an ubuntu machine to an admin node run this command. This script uses git over SSH, so make sure the admin node have ssh keys created, and those keys have been added to github The script does the following - Installs openTofu - Installs git - Downloads this repository - Installs talosctl - Installs kubectl - Installs helm - Installs flux curl --proto '=https' --tlsv1.2 -fsSL https://raw.githubusercontent.com/AndersBallegaard/homelab-k8s/refs/heads/main/admin/prepare_admin_node.sh | bash","title":"Prepare admin node"},{"location":"infra/deployment/prepare_admin_node/#prepare-admin-node","text":"To convert an ubuntu machine to an admin node run this command. This script uses git over SSH, so make sure the admin node have ssh keys created, and those keys have been added to github The script does the following - Installs openTofu - Installs git - Downloads this repository - Installs talosctl - Installs kubectl - Installs helm - Installs flux curl --proto '=https' --tlsv1.2 -fsSL https://raw.githubusercontent.com/AndersBallegaard/homelab-k8s/refs/heads/main/admin/prepare_admin_node.sh | bash","title":"Prepare admin node"},{"location":"infra/deployment/prepare_proxmox/","text":"Prepare proxmox for cloudinit In case proxmox isn't ready for cloudinit of talos, this is how to set it up. Currently this is done from the proxmox UI as a one of thing # Download a talos image from the talos image factory, it needs to be a nocloud image # The image should be placed under the name \"talos-nocloud-amd64.iso\" in the shared vm datastore # Create a VM with the 1g disk on shared storage, and attach the iso. Ram and CPU doesn't matter # Don't start the VM # The vm MUST be named talos-template, and should have the ID 9100 # CPU Type should be host for best performace # Disk size is intentionally below recormendations for faster template deployments, cloud init will resize to whatever the VM is speced for # # Convert VM to template","title":"Prepare proxmox"},{"location":"infra/deployment/prepare_proxmox/#prepare-proxmox-for-cloudinit","text":"In case proxmox isn't ready for cloudinit of talos, this is how to set it up. Currently this is done from the proxmox UI as a one of thing # Download a talos image from the talos image factory, it needs to be a nocloud image # The image should be placed under the name \"talos-nocloud-amd64.iso\" in the shared vm datastore # Create a VM with the 1g disk on shared storage, and attach the iso. Ram and CPU doesn't matter # Don't start the VM # The vm MUST be named talos-template, and should have the ID 9100 # CPU Type should be host for best performace # Disk size is intentionally below recormendations for faster template deployments, cloud init will resize to whatever the VM is speced for # # Convert VM to template","title":"Prepare proxmox for cloudinit"},{"location":"infra/deployment/setup_cluster/","text":"Provision Cluster # Set env variables export TF_VAR_cloudflare_api_token=READ_WRITE_TOKEN export TF_VAR_cloudflare_zone_id=DOMAIN_ZONEID export TF_VAR_vm_user_username=anders export TF_VAR_vm_user_sshkey=\"SSH PUBLIC ID for authentication\" export TF_VAR_proxmox_username=USERNAME@pam export TF_VAR_proxmox_password=PROXMOX_PASSWORD export TF_VAR_proxmox_api_url=https://PROXMOX_SERVER:8006/api2/json # Initialize openTofu cd admin tofu init # Bootstrap the cluster ./patch_infra.sh Setup Cillium CNI # Create BGP Password kubectl create secret generic -n kube-system --type=string bgp-auth-secret --from-literal=password=REPLACEWITHPASSWORD Setup fluxCD flux bootstrap github \\ --token-auth \\ --owner=andersballegaard \\ --repository=homelab-k8s \\ --branch=main \\ --path=cluster \\ --personal","title":"Setup cluster"},{"location":"infra/deployment/setup_cluster/#provision-cluster","text":"# Set env variables export TF_VAR_cloudflare_api_token=READ_WRITE_TOKEN export TF_VAR_cloudflare_zone_id=DOMAIN_ZONEID export TF_VAR_vm_user_username=anders export TF_VAR_vm_user_sshkey=\"SSH PUBLIC ID for authentication\" export TF_VAR_proxmox_username=USERNAME@pam export TF_VAR_proxmox_password=PROXMOX_PASSWORD export TF_VAR_proxmox_api_url=https://PROXMOX_SERVER:8006/api2/json # Initialize openTofu cd admin tofu init # Bootstrap the cluster ./patch_infra.sh","title":"Provision Cluster"},{"location":"infra/deployment/setup_cluster/#setup-cillium-cni","text":"# Create BGP Password kubectl create secret generic -n kube-system --type=string bgp-auth-secret --from-literal=password=REPLACEWITHPASSWORD","title":"Setup Cillium CNI"},{"location":"infra/deployment/setup_cluster/#setup-fluxcd","text":"flux bootstrap github \\ --token-auth \\ --owner=andersballegaard \\ --repository=homelab-k8s \\ --branch=main \\ --path=cluster \\ --personal","title":"Setup fluxCD"},{"location":"infra/networking/overview/","text":"Overview The kubernetes lab uses Cilium as the CNI, in this deployment Cilium is running IPv6 only in native-routing mode with BGP peerings to two virtual VYOS routers acting as essentially a mix between TOR switches and a CE router for my personal ASN AS201911. Cilium was chosen as the CNI due to the following reasons - Excelent BGP support - Interesting observability tools though Hubble - High performance due to eBPF - Great support for network policies - Support for advanced networking features i might want to checkout like srv6 in k8s - Built in gateway API and ingress See diagram below for a vistual representation Addressing Due to running IPv6 only in my own ASN, i have assigned a /48 network to this k8s lab. Some of the plan looks the way it does due to previously being part of a shared /48 network with other lab infra, cleaning up the IP plan is todo point for a time where the cluster is down anyways. The addressing plan looks as follows Prefix Location Use 2a0e:97c0:ae3::/48 Lab Assigned Aggregate 2a0e:97c0:ae3:c100::/112 Cluster K8S Service subnet 2a0e:97c0:ae3:c200::/56 Cluster K8S PodCIDR 2a0e:97c0:ae3:c401::/64 Cluster K8S Cilium LB IP pool 2a0e:97c0:ae3:fff0::/64 Vlan 2502 - K8S Node network Node IP's 2a0e:97c0:ae3:ffff::/64 Loopback aggregates Assigned Aggregate 2a0e:97c0:ae3:ffff::1/128 r01.k8s.srv6.dk MGMT Loopback 2a0e:97c0:ae3:ffff::2/128 r02.k8s.srv6.dk MGMT Loopback Cilium configuration Cilium is installed doing setup of the cluster, for more information about that see Cluster setup and patch_infra.sh script For configuration related parameters, I am using a kustomization deployed by fluxcd to manage it. This choice have been made in order to have as much declaritive, easy to manage configuration as posible. Loadbalancer pools The cluster needs external IP's to assign to any service of the type loadBalancer, given i have an ipv6 only cluster with my own address space, the most obivious choice was to use global ipv6 in this case, but you can absolutly find other solutions if that's not posible for your setup. # cluster/infra/cilium/lb-ipam.yaml apiVersion: \"cilium.io/v2alpha1\" kind: CiliumLoadBalancerIPPool metadata: name: \"lb-pool\" spec: blocks: - cidr: \"2a0e:97c0:ae3:c401::/64\" BGP Configuration Let's look at the general BGP setup. There is quite a lot going on here, but in it basicly just does the following * Creates a BGP \"Cluster\" called cilium-bgp, and provisions it on all nodes * Specifies that the local-as is 65500 * Sets up peerings to R01.k8s.srv6.dk and R02.k8s.srv6.dk * Creates a CiliumBGPPeerConfig (Peer group) specifying that ipv6 unicast is the only family enabled, and setting a password # cluster/infra/cilium/bgp.yaml apiVersion: cilium.io/v2alpha1 kind: CiliumBGPClusterConfig metadata: name: cilium-bgp namespace: kube-system spec: nodeSelector: matchLabels: kubernetes.io/os: linux bgpInstances: - name: \"instance-65500\" localASN: 65500 peers: - name: \"r01\" peerASN: 65666 peerAddress: 2a0e:97c0:ae3:fff0::1 peerConfigRef: name: \"cilium-peer\" - name: \"r02\" peerASN: 65666 peerAddress: 2a0e:97c0:ae3:fff0::2 peerConfigRef: name: \"cilium-peer\" --- apiVersion: cilium.io/v2alpha1 kind: CiliumBGPPeerConfig metadata: name: cilium-peer namespace: kube-system spec: authSecretRef: bgp-auth-secret timers: holdTimeSeconds: 9 keepAliveTimeSeconds: 3 ebgpMultihop: 4 gracefulRestart: enabled: true restartTimeSeconds: 15 families: - afi: ipv6 safi: unicast advertisements: matchLabels: advertise: bgp NOTE: This uses the BGP secret set during cluster setup, please make sure the password accually matches. NOTE: Under cluster setup anotate-nodes.sh was executed in the background, this script sets router-id's on a list of nodes. If some nodes won't peer check that a router-id have been set Create annoncements Ok, we are not quite done with the BGP configuration, we still needs to specify what to announce. In my example due to running cilium in native-routing mode, i want to advertise everything i know. For this reason i am creating two CiliumBGPAdvertisement objects bellow advertising the following ExternalIP LoadBalancerIP ClusterIP PodCIDR # cluster/infra/cilium/advertisements.yaml --- apiVersion: cilium.io/v2alpha1 kind: CiliumBGPAdvertisement metadata: labels: advertise: bgp name: bgp-advertisements spec: advertisements: - advertisementType: Service selector: matchExpressions: - key: somekey # For some reason this works, and things break if i remove it, no this key does not exist anywhere operator: NotIn values: - never-used-value service: addresses: - ExternalIP - LoadBalancerIP - ClusterIP --- apiVersion: cilium.io/v2alpha1 kind: CiliumBGPAdvertisement metadata: name: bgp-advertisements-podcidr labels: advertise: bgp spec: advertisements: - advertisementType: \"PodCIDR\" attributes: communities: standard: [ \"65000:99\" ] localPreference: 99 Building the kustomization Let's put it all togther in a simple kustomization # cluster/infra/cilium/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization metadata: name: cillium-config resources: - lb-ipam.yaml - bgp.yaml - advertisements.yaml Router configuration See VYOS Configuration This page is autogenerated, do not edit it directly see this for more information","title":"Overview"},{"location":"infra/networking/overview/#overview","text":"The kubernetes lab uses Cilium as the CNI, in this deployment Cilium is running IPv6 only in native-routing mode with BGP peerings to two virtual VYOS routers acting as essentially a mix between TOR switches and a CE router for my personal ASN AS201911. Cilium was chosen as the CNI due to the following reasons - Excelent BGP support - Interesting observability tools though Hubble - High performance due to eBPF - Great support for network policies - Support for advanced networking features i might want to checkout like srv6 in k8s - Built in gateway API and ingress See diagram below for a vistual representation","title":"Overview"},{"location":"infra/networking/overview/#addressing","text":"Due to running IPv6 only in my own ASN, i have assigned a /48 network to this k8s lab. Some of the plan looks the way it does due to previously being part of a shared /48 network with other lab infra, cleaning up the IP plan is todo point for a time where the cluster is down anyways. The addressing plan looks as follows Prefix Location Use 2a0e:97c0:ae3::/48 Lab Assigned Aggregate 2a0e:97c0:ae3:c100::/112 Cluster K8S Service subnet 2a0e:97c0:ae3:c200::/56 Cluster K8S PodCIDR 2a0e:97c0:ae3:c401::/64 Cluster K8S Cilium LB IP pool 2a0e:97c0:ae3:fff0::/64 Vlan 2502 - K8S Node network Node IP's 2a0e:97c0:ae3:ffff::/64 Loopback aggregates Assigned Aggregate 2a0e:97c0:ae3:ffff::1/128 r01.k8s.srv6.dk MGMT Loopback 2a0e:97c0:ae3:ffff::2/128 r02.k8s.srv6.dk MGMT Loopback","title":"Addressing"},{"location":"infra/networking/overview/#cilium-configuration","text":"Cilium is installed doing setup of the cluster, for more information about that see Cluster setup and patch_infra.sh script For configuration related parameters, I am using a kustomization deployed by fluxcd to manage it. This choice have been made in order to have as much declaritive, easy to manage configuration as posible.","title":"Cilium configuration"},{"location":"infra/networking/overview/#loadbalancer-pools","text":"The cluster needs external IP's to assign to any service of the type loadBalancer, given i have an ipv6 only cluster with my own address space, the most obivious choice was to use global ipv6 in this case, but you can absolutly find other solutions if that's not posible for your setup. # cluster/infra/cilium/lb-ipam.yaml apiVersion: \"cilium.io/v2alpha1\" kind: CiliumLoadBalancerIPPool metadata: name: \"lb-pool\" spec: blocks: - cidr: \"2a0e:97c0:ae3:c401::/64\"","title":"Loadbalancer pools"},{"location":"infra/networking/overview/#bgp-configuration","text":"Let's look at the general BGP setup. There is quite a lot going on here, but in it basicly just does the following * Creates a BGP \"Cluster\" called cilium-bgp, and provisions it on all nodes * Specifies that the local-as is 65500 * Sets up peerings to R01.k8s.srv6.dk and R02.k8s.srv6.dk * Creates a CiliumBGPPeerConfig (Peer group) specifying that ipv6 unicast is the only family enabled, and setting a password # cluster/infra/cilium/bgp.yaml apiVersion: cilium.io/v2alpha1 kind: CiliumBGPClusterConfig metadata: name: cilium-bgp namespace: kube-system spec: nodeSelector: matchLabels: kubernetes.io/os: linux bgpInstances: - name: \"instance-65500\" localASN: 65500 peers: - name: \"r01\" peerASN: 65666 peerAddress: 2a0e:97c0:ae3:fff0::1 peerConfigRef: name: \"cilium-peer\" - name: \"r02\" peerASN: 65666 peerAddress: 2a0e:97c0:ae3:fff0::2 peerConfigRef: name: \"cilium-peer\" --- apiVersion: cilium.io/v2alpha1 kind: CiliumBGPPeerConfig metadata: name: cilium-peer namespace: kube-system spec: authSecretRef: bgp-auth-secret timers: holdTimeSeconds: 9 keepAliveTimeSeconds: 3 ebgpMultihop: 4 gracefulRestart: enabled: true restartTimeSeconds: 15 families: - afi: ipv6 safi: unicast advertisements: matchLabels: advertise: bgp NOTE: This uses the BGP secret set during cluster setup, please make sure the password accually matches. NOTE: Under cluster setup anotate-nodes.sh was executed in the background, this script sets router-id's on a list of nodes. If some nodes won't peer check that a router-id have been set","title":"BGP Configuration"},{"location":"infra/networking/overview/#create-annoncements","text":"Ok, we are not quite done with the BGP configuration, we still needs to specify what to announce. In my example due to running cilium in native-routing mode, i want to advertise everything i know. For this reason i am creating two CiliumBGPAdvertisement objects bellow advertising the following ExternalIP LoadBalancerIP ClusterIP PodCIDR # cluster/infra/cilium/advertisements.yaml --- apiVersion: cilium.io/v2alpha1 kind: CiliumBGPAdvertisement metadata: labels: advertise: bgp name: bgp-advertisements spec: advertisements: - advertisementType: Service selector: matchExpressions: - key: somekey # For some reason this works, and things break if i remove it, no this key does not exist anywhere operator: NotIn values: - never-used-value service: addresses: - ExternalIP - LoadBalancerIP - ClusterIP --- apiVersion: cilium.io/v2alpha1 kind: CiliumBGPAdvertisement metadata: name: bgp-advertisements-podcidr labels: advertise: bgp spec: advertisements: - advertisementType: \"PodCIDR\" attributes: communities: standard: [ \"65000:99\" ] localPreference: 99","title":"Create annoncements"},{"location":"infra/networking/overview/#building-the-kustomization","text":"Let's put it all togther in a simple kustomization # cluster/infra/cilium/kustomization.yaml apiVersion: kustomize.config.k8s.io/v1beta1 kind: Kustomization metadata: name: cillium-config resources: - lb-ipam.yaml - bgp.yaml - advertisements.yaml","title":"Building the kustomization"},{"location":"infra/networking/overview/#router-configuration","text":"See VYOS Configuration This page is autogenerated, do not edit it directly see this for more information","title":"Router configuration"},{"location":"infra/networking/vyos/","text":"Vyos Two VYOS Routers are used to provide external connectivity to the cluster, eventually this should be automated using cloud-init for VYOS, but right now i can't be bothered, so this is how they are setup Specs Both of the virtual routers have these specs, and are being run on proxmox. The specs are probably subject to change depending on the volume of traffic. But even with a full IPv6 internet routing table being annouced to them from AS201911 they seem happy with these modest specs. 1 core 2GB Ram 1 NIC in AS201911 Transit network 1 NIC in K8S network General router baseline Setup accounts Setup update checking Disable SSH Password auth # Unless you want me, and only me to login to your router, i would sudgest changing this key set system login user anders authentication public-keys JUMPHOST key 'AAAAB3NzaC1yc2EAAAADAQABAAABgQC8LOd5EgLmJiqWlikJMBNqn92c1TMyDYYJybmMRwu3ysOcf5jWwzVx1d6jZOPboH241Q3Rg59AtqJpFLEDis0myVvdLfcZa88DsjhfFUgDA4LMatItoUpqj2xKwJsTHKWjaSL5zVGv1rXOnMv3CA90Cye7NrEP11AxrLeCawpsQC/Gwcrc7JLoKrYK1bz2zPI9WwPonKhOUC0zfpAX9G9mj3Ybp7aogYAsBweoFw4X533usswFDkPZl9SYnetufEZ/XGgl3TFoDETsQ4Ddztazu9snPRwlIyAq2Lv1+tU2hc2lKu0AWv4zPehEdMz8dXodB1bfkD63Rpy004Naa+tboJnJQUUpfZqqVEpI0byp7t64suAU1IvmgZo02A3kg6m+HJuMU6k1hAbrgXttRWR/fRDSvdGydPiyi+8mSvVYrstcpH63mvMw2gVOkoDeBzP+ClejkCoTLuXSb38reAivG0j02FyyoWRrBumfBHJb6lC8hUHQi04DfX2B/G+0UVM=' set system login user anders authentication public-keys JUMPHOST type 'ssh-rsa' set system name-server '2606:4700:4700::1111' set system update-check auto-check set system update-check url 'https://raw.githubusercontent.com/vyos/vyos-nightly-build/refs/heads/current/version.json' set service ssh disable-password-authentication set system domain-name 'k8s.srv6.dk' set interfaces ethernet eth0 description 'AS201911_TRANSIT' set interfaces ethernet eth1 description 'K8S_NETWORK' Firewall configuration Since these servers are technicly directly connected to the internet, a few firewall rules to help secure the servers are in order. These are the rules i have chosen to create for now, mixing security with ease of administration. Obiviously, you probably want some thigher rules in enviorments where security is more of a concern I have other firewalls for the rest of my ASN so i generally trust anything from one of my ASN's IP's I am ok with permitting all outgoing traffic from the k8s cluster I want to allow port 80 and 443 in from the entire internet, for services. Obiviously ICMPv6, BGP, link local should also be allowed given the setup set firewall ipv6 input filter default-action 'drop' set firewall ipv6 input filter default-log set firewall ipv6 input filter rule 1 action 'accept' set firewall ipv6 input filter rule 1 description 'Allow everything from AS201911 (my networks)' set firewall ipv6 input filter rule 1 source address '2a0e:97c0:ae0::/44' set firewall ipv6 input filter rule 2 action 'accept' set firewall ipv6 input filter rule 2 description 'Accept BGP' set firewall ipv6 input filter rule 2 destination port '179' set firewall ipv6 input filter rule 2 protocol 'tcp' set firewall ipv6 input filter rule 3 action 'accept' set firewall ipv6 input filter rule 3 description 'Accept link-local' set firewall ipv6 input filter rule 3 source address 'fe80::/16' set firewall ipv6 input filter rule 4 action 'accept' set firewall ipv6 input filter rule 4 protocol 'ipv6-icmp' set firewall ipv6 input filter rule 10 action 'accept' set firewall ipv6 input filter rule 10 description 'Allow anything out from K8S' set firewall ipv6 input filter rule 10 inbound-interface name 'eth1' set firewall ipv6 input filter rule 30 action 'accept' set firewall ipv6 input filter rule 30 description 'Allow HTTP/HTTPS to K8S' set firewall ipv6 input filter rule 30 destination address '2a0e:97c0:ae3::/48' set firewall ipv6 input filter rule 30 destination port 'http,https' set firewall ipv6 input filter rule 30 protocol 'tcp' BGP Config The BGP config here is quite simple, create peerings to AS201911, and create a listen group to dynamicly allow in K8S nodes. There is no reason to send anything other than a default route to the nodes, so we do that and limit the memory consumption to have more room for containers. set policy prefix-list6 DEFAULT rule 1 action 'permit' set policy prefix-list6 DEFAULT rule 1 prefix '::/0' set policy route-map EXPORT_DEFAULT rule 10 action 'permit' set policy route-map EXPORT_DEFAULT rule 10 match ipv6 address prefix-list 'DEFAULT' set policy route-map EXPORT_DEFAULT rule 20 action 'deny' set protocols bgp address-family ipv6-unicast redistribute connected set protocols bgp listen range 2a0e:97c0:ae3:fff0::/64 peer-group 'K8S' set protocols bgp neighbor 2a0e:97c0:ae0:102::1 address-family ipv6-unicast set protocols bgp neighbor 2a0e:97c0:ae0:102::1 remote-as '201911' set protocols bgp neighbor 2a0e:97c0:ae0:102::2 address-family ipv6-unicast set protocols bgp neighbor 2a0e:97c0:ae0:102::2 remote-as '201911' set protocols bgp peer-group K8S address-family ipv6-unicast route-map export 'EXPORT_DEFAULT' set protocols bgp peer-group K8S password 'REPLACEWITHPASSWORD' set protocols bgp peer-group K8S remote-as 'external' set protocols bgp system-as '65666' Router specific configuration for r01.k8s.srv6.dk set protocols bgp parameters router-id '127.0.1.1' set interfaces ethernet eth0 address '2a0e:97c0:ae0:102::c001/64' set interfaces ethernet eth1 address '2a0e:97c0:ae3:fff0::1/64' set interfaces loopback lo address '2a0e:97c0:ae3:ffff::1/128' set system host-name 'r01' Router specific configuration for r02.k8s.srv6.dk To be build when the direction of everything is a bit more stable set protocols bgp parameters router-id '127.0.1.2' set interfaces ethernet eth0 address '2a0e:97c0:ae0:102::c002/64' set interfaces ethernet eth1 address '2a0e:97c0:ae3:fff0::2/64' set interfaces loopback lo address '2a0e:97c0:ae3:ffff::2/128' set system host-name 'r02'","title":"VYOS Configuration"},{"location":"infra/networking/vyos/#vyos","text":"Two VYOS Routers are used to provide external connectivity to the cluster, eventually this should be automated using cloud-init for VYOS, but right now i can't be bothered, so this is how they are setup","title":"Vyos"},{"location":"infra/networking/vyos/#specs","text":"Both of the virtual routers have these specs, and are being run on proxmox. The specs are probably subject to change depending on the volume of traffic. But even with a full IPv6 internet routing table being annouced to them from AS201911 they seem happy with these modest specs. 1 core 2GB Ram 1 NIC in AS201911 Transit network 1 NIC in K8S network","title":"Specs"},{"location":"infra/networking/vyos/#general-router-baseline","text":"Setup accounts Setup update checking Disable SSH Password auth # Unless you want me, and only me to login to your router, i would sudgest changing this key set system login user anders authentication public-keys JUMPHOST key 'AAAAB3NzaC1yc2EAAAADAQABAAABgQC8LOd5EgLmJiqWlikJMBNqn92c1TMyDYYJybmMRwu3ysOcf5jWwzVx1d6jZOPboH241Q3Rg59AtqJpFLEDis0myVvdLfcZa88DsjhfFUgDA4LMatItoUpqj2xKwJsTHKWjaSL5zVGv1rXOnMv3CA90Cye7NrEP11AxrLeCawpsQC/Gwcrc7JLoKrYK1bz2zPI9WwPonKhOUC0zfpAX9G9mj3Ybp7aogYAsBweoFw4X533usswFDkPZl9SYnetufEZ/XGgl3TFoDETsQ4Ddztazu9snPRwlIyAq2Lv1+tU2hc2lKu0AWv4zPehEdMz8dXodB1bfkD63Rpy004Naa+tboJnJQUUpfZqqVEpI0byp7t64suAU1IvmgZo02A3kg6m+HJuMU6k1hAbrgXttRWR/fRDSvdGydPiyi+8mSvVYrstcpH63mvMw2gVOkoDeBzP+ClejkCoTLuXSb38reAivG0j02FyyoWRrBumfBHJb6lC8hUHQi04DfX2B/G+0UVM=' set system login user anders authentication public-keys JUMPHOST type 'ssh-rsa' set system name-server '2606:4700:4700::1111' set system update-check auto-check set system update-check url 'https://raw.githubusercontent.com/vyos/vyos-nightly-build/refs/heads/current/version.json' set service ssh disable-password-authentication set system domain-name 'k8s.srv6.dk' set interfaces ethernet eth0 description 'AS201911_TRANSIT' set interfaces ethernet eth1 description 'K8S_NETWORK'","title":"General router baseline"},{"location":"infra/networking/vyos/#firewall-configuration","text":"Since these servers are technicly directly connected to the internet, a few firewall rules to help secure the servers are in order. These are the rules i have chosen to create for now, mixing security with ease of administration. Obiviously, you probably want some thigher rules in enviorments where security is more of a concern I have other firewalls for the rest of my ASN so i generally trust anything from one of my ASN's IP's I am ok with permitting all outgoing traffic from the k8s cluster I want to allow port 80 and 443 in from the entire internet, for services. Obiviously ICMPv6, BGP, link local should also be allowed given the setup set firewall ipv6 input filter default-action 'drop' set firewall ipv6 input filter default-log set firewall ipv6 input filter rule 1 action 'accept' set firewall ipv6 input filter rule 1 description 'Allow everything from AS201911 (my networks)' set firewall ipv6 input filter rule 1 source address '2a0e:97c0:ae0::/44' set firewall ipv6 input filter rule 2 action 'accept' set firewall ipv6 input filter rule 2 description 'Accept BGP' set firewall ipv6 input filter rule 2 destination port '179' set firewall ipv6 input filter rule 2 protocol 'tcp' set firewall ipv6 input filter rule 3 action 'accept' set firewall ipv6 input filter rule 3 description 'Accept link-local' set firewall ipv6 input filter rule 3 source address 'fe80::/16' set firewall ipv6 input filter rule 4 action 'accept' set firewall ipv6 input filter rule 4 protocol 'ipv6-icmp' set firewall ipv6 input filter rule 10 action 'accept' set firewall ipv6 input filter rule 10 description 'Allow anything out from K8S' set firewall ipv6 input filter rule 10 inbound-interface name 'eth1' set firewall ipv6 input filter rule 30 action 'accept' set firewall ipv6 input filter rule 30 description 'Allow HTTP/HTTPS to K8S' set firewall ipv6 input filter rule 30 destination address '2a0e:97c0:ae3::/48' set firewall ipv6 input filter rule 30 destination port 'http,https' set firewall ipv6 input filter rule 30 protocol 'tcp'","title":"Firewall configuration"},{"location":"infra/networking/vyos/#bgp-config","text":"The BGP config here is quite simple, create peerings to AS201911, and create a listen group to dynamicly allow in K8S nodes. There is no reason to send anything other than a default route to the nodes, so we do that and limit the memory consumption to have more room for containers. set policy prefix-list6 DEFAULT rule 1 action 'permit' set policy prefix-list6 DEFAULT rule 1 prefix '::/0' set policy route-map EXPORT_DEFAULT rule 10 action 'permit' set policy route-map EXPORT_DEFAULT rule 10 match ipv6 address prefix-list 'DEFAULT' set policy route-map EXPORT_DEFAULT rule 20 action 'deny' set protocols bgp address-family ipv6-unicast redistribute connected set protocols bgp listen range 2a0e:97c0:ae3:fff0::/64 peer-group 'K8S' set protocols bgp neighbor 2a0e:97c0:ae0:102::1 address-family ipv6-unicast set protocols bgp neighbor 2a0e:97c0:ae0:102::1 remote-as '201911' set protocols bgp neighbor 2a0e:97c0:ae0:102::2 address-family ipv6-unicast set protocols bgp neighbor 2a0e:97c0:ae0:102::2 remote-as '201911' set protocols bgp peer-group K8S address-family ipv6-unicast route-map export 'EXPORT_DEFAULT' set protocols bgp peer-group K8S password 'REPLACEWITHPASSWORD' set protocols bgp peer-group K8S remote-as 'external' set protocols bgp system-as '65666'","title":"BGP Config"},{"location":"infra/networking/vyos/#router-specific-configuration-for-r01k8ssrv6dk","text":"set protocols bgp parameters router-id '127.0.1.1' set interfaces ethernet eth0 address '2a0e:97c0:ae0:102::c001/64' set interfaces ethernet eth1 address '2a0e:97c0:ae3:fff0::1/64' set interfaces loopback lo address '2a0e:97c0:ae3:ffff::1/128' set system host-name 'r01'","title":"Router specific configuration for r01.k8s.srv6.dk"},{"location":"infra/networking/vyos/#router-specific-configuration-for-r02k8ssrv6dk","text":"To be build when the direction of everything is a bit more stable set protocols bgp parameters router-id '127.0.1.2' set interfaces ethernet eth0 address '2a0e:97c0:ae0:102::c002/64' set interfaces ethernet eth1 address '2a0e:97c0:ae3:fff0::2/64' set interfaces loopback lo address '2a0e:97c0:ae3:ffff::2/128' set system host-name 'r02'","title":"Router specific configuration for r02.k8s.srv6.dk"},{"location":"troubleshooting/port-forwarding/","text":"Port forwarding For troubleshooting purposes it is often practical to port forward a service in k8s. This is also the only way to access certine admin interfaces that aren't exposed with an ingress Generic service This is the general way to expose any service with a port-forward. In a step to reduce the use of legacy IP, binding to ipv6 is forced in this example, note you might want to bind to ::1 instead of :: kubectl -n NAMESPACE port-forward svc/SERVICE_NAME PORT:PORT --address=\"::\" Capacitor If troubleshooting fluxcd, it might be nice to have a more visual representation of what services are in fluxcd. For this purpose a capacitor deployment exists, but isn't exposed via any ingress for security reasons, to forward it run the following command kubectl -n flux-system port-forward svc/capacitor 9000:9000 --address=\"::1\" CEPH dashboard When troubleshooting ceph/rook, it might be nice to use the ceph dashboard, it can be exposed using the following command kubectl -n rook-ceph port-forward svc/rook-ceph-mgr-dashboard 7000:7000 --address=\"::\" In order to login use the username \"admin\", and the password provided by the following command. kubectl get secret -n rook-ceph -o jsonpath='{.data.password}' rook-ceph-dashboard-password | base64 -d","title":"K8S Port forwarding"},{"location":"troubleshooting/port-forwarding/#port-forwarding","text":"For troubleshooting purposes it is often practical to port forward a service in k8s. This is also the only way to access certine admin interfaces that aren't exposed with an ingress","title":"Port forwarding"},{"location":"troubleshooting/port-forwarding/#generic-service","text":"This is the general way to expose any service with a port-forward. In a step to reduce the use of legacy IP, binding to ipv6 is forced in this example, note you might want to bind to ::1 instead of :: kubectl -n NAMESPACE port-forward svc/SERVICE_NAME PORT:PORT --address=\"::\"","title":"Generic service"},{"location":"troubleshooting/port-forwarding/#capacitor","text":"If troubleshooting fluxcd, it might be nice to have a more visual representation of what services are in fluxcd. For this purpose a capacitor deployment exists, but isn't exposed via any ingress for security reasons, to forward it run the following command kubectl -n flux-system port-forward svc/capacitor 9000:9000 --address=\"::1\"","title":"Capacitor"},{"location":"troubleshooting/port-forwarding/#ceph-dashboard","text":"When troubleshooting ceph/rook, it might be nice to use the ceph dashboard, it can be exposed using the following command kubectl -n rook-ceph port-forward svc/rook-ceph-mgr-dashboard 7000:7000 --address=\"::\" In order to login use the username \"admin\", and the password provided by the following command. kubectl get secret -n rook-ceph -o jsonpath='{.data.password}' rook-ceph-dashboard-password | base64 -d","title":"CEPH dashboard"}]}